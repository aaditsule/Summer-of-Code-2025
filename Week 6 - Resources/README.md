# Long-Short Term Memory (LSTM)

LSTM (Long-Short Term Memory) networks are a type of Recurrent Neural Network (RNN) capable of learning long-term dependencies. They are particularly effective for sequential data and time-series predictions. Unlike traditional RNNs, which struggle with long-term dependencies due to vanishing or exploding gradients, LSTMs are specifically designed to avoid these issues.

---

## Key Concepts

### 1. **Memory Cell**
The LSTM's memory cell is its core component. Unlike traditional RNNs, which pass hidden states from one time step to another, LSTMs maintain a cell state that flows through the network with minimal changes. This allows LSTMs to retain important information over long sequences while discarding irrelevant details.

### 2. **Gates**
LSTMs regulate the flow of information using three types of gates. These gates are implemented using sigmoid activation functions, which output values between 0 and 1, where 0 represents complete exclusion and 1 represents complete inclusion of the information.

#### (a) Forget Gate
The forget gate determines which parts of the cell state should be discarded. It uses the previous hidden state ($`\mathbf{h}_{t-1}`$) and the current input ($`\mathbf{x}_t`$) as inputs:

$`
\mathbf{f}_t = \sigma(\mathbf{W}_f [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_f)
`$

Here:
- $`\mathbf{W}_f`$: Weight matrix for the forget gate.
- $`\mathbf{b}_f`$: Bias vector for the forget gate.
- $`\sigma`$: Sigmoid activation function.

#### (b) Input Gate
The input gate decides which new information to store in the cell state. It has two components:

1. **Input modulation:** Generates candidate values to be added to the cell state.
$`
\tilde{\mathbf{C}}_t = \tanh(\mathbf{W}_C [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_C)
`$

2. **Input gate activation:** Determines how much of the candidate values to include.
$`
\mathbf{i}_t = \sigma(\mathbf{W}_i [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_i)
`$

#### (c) Output Gate
The output gate controls what part of the cell state to output as the hidden state for the current time step:

$`
\mathbf{o}_t = \sigma(\mathbf{W}_o [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_o)
`$

The hidden state $`\mathbf{h}_t`$ is calculated as:
$`
\mathbf{h}_t = \mathbf{o}_t \odot \tanh(\mathbf{C}_t)
`$

### 3. **Cell State Update**
The cell state $`\mathbf{C}_t`$ is updated using a combination of the forget gate and input gate:

$`
\mathbf{C}_t = \mathbf{f}_t \odot \mathbf{C}_{t-1} + \mathbf{i}_t \odot \tilde{\mathbf{C}}_t
`$

Here:
- $`\odot`$: Element-wise multiplication.
- $`\tilde{\mathbf{C}}_t`$: Candidate cell state generated by the input modulation.

---


## Tutorials and Resources

### Go through the following websites:
1. [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
2. [LSTM and its diagrams](https://towardsdatascience.com/understanding-lstm-and-its-diagrams-37e2f46f1714)
3. [LSTM on GeeksforGeeks](https://en.wikipedia.org/wiki/Long_short-term_memory)

### YouTube Videos:
1. [Recurrent Neural Networks (RNNs)](https://www.youtube.com/watch?v=8HyCNIVRbSU)
2. [LSTM Networks](https://www.youtube.com/watch?v=WCUNPb-5EYI)
3. [Deep Learning](https://www.youtube.com/watch?v=QciIcRxJvsM)

---

## Example Code
Here is a simple example of using LSTM with Python's TensorFlow library:

```python
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# Generate dummy data
X = np.random.random((100, 10, 1))  # 100 samples, 10 time steps, 1 feature
y = np.random.random((100, 1))      # 100 target values

# Define model
model = Sequential([
    LSTM(50, return_sequences=True, input_shape=(10, 1)),
    LSTM(50, return_sequences=False),
    Dense(1)
])

# Compile model
model.compile(optimizer='adam', loss='mean_squared_error')

# Train model
model.fit(X, y, epochs=10, batch_size=16)

# Summary
model.summary()
```

This example demonstrates a simple LSTM model for regression tasks using dummy data. You can adapt it for real-world datasets and more complex use cases.

---
